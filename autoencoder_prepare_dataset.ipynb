{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "from scipy import ndimage\n",
    "from modules.edge_detection import detect_edges, scale_data, scale_semantic_map\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects_path = \"MnM2/dataset\"\n",
    "\n",
    "# resolution = 256\n",
    "\n",
    "# all_subjects = [d for d in os.listdir(subjects_path) if os.path.isdir(os.path.join(subjects_path, d))]\n",
    "\n",
    "# sorted_subjects = sorted(all_subjects)\n",
    "\n",
    "# processed_data_path = os.path.join(\"processed_data\")\n",
    "# os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "# # Dictionary to store the index mapping\n",
    "# index_mapping = {}\n",
    "# current_index = 0\n",
    "\n",
    "# for sub in sorted_subjects:\n",
    "#     subject_path = os.path.join(subjects_path, sub)\n",
    "#     subject_files = [d for d in os.listdir(subject_path) if d.endswith(('.nii', '.nii.gz'))]\n",
    "#     sorted_subject_files = sorted(subject_files)\n",
    "\n",
    "#     print(sorted_subject_files)\n",
    "\n",
    "#     normalized_images = []\n",
    "#     edges_images = []\n",
    "#     semantic_maps = []\n",
    "\n",
    "#     for file in sorted_subject_files:\n",
    "#         parts = file.split('_')\n",
    "#         file_type = '_'.join(parts[1:-1])\n",
    "#         file_type += '_' + parts[-1].split('.')[0]\n",
    "\n",
    "#         if file_type == \"LA_ED\":\n",
    "#             img = nib.load(os.path.join(subject_path, file))\n",
    "#             data = img.get_fdata()\n",
    "\n",
    "#             resized_data = ndimage.zoom(data, (resolution/data.shape[0], resolution/data.shape[1], 1))\n",
    "\n",
    "#             scale = scale_data(resized_data)\n",
    "\n",
    "#             edges = detect_edges(resized_data)\n",
    "\n",
    "#             normalized_images.append(scale[:,:,0])\n",
    "\n",
    "#             edges_images.append(edges)\n",
    "\n",
    "#         elif file_type == \"LA_ED_gt\":\n",
    "#             img = nib.load(os.path.join(subject_path, file))\n",
    "#             data = img.get_fdata()\n",
    "\n",
    "#             resized_data = scale_semantic_map(data, (resolution, resolution))\n",
    "\n",
    "#             semantic_maps.append(resized_data[:,:,0])\n",
    "\n",
    "#         elif file_type == \"LA_ES\":\n",
    "#             img = nib.load(os.path.join(subject_path, file))\n",
    "#             data = img.get_fdata()\n",
    "\n",
    "#             resized_data = ndimage.zoom(data, (resolution/data.shape[0], resolution/data.shape[1], 1))\n",
    "\n",
    "#             scale = scale_data(resized_data)\n",
    "\n",
    "#             edges = detect_edges(resized_data)\n",
    "\n",
    "#             normalized_images.append(scale[:,:,0])\n",
    "\n",
    "#             edges_images.append(edges)\n",
    "\n",
    "#         elif file_type == \"LA_ES_gt\":\n",
    "#             img = nib.load(os.path.join(subject_path, file))\n",
    "#             data = img.get_fdata()\n",
    "\n",
    "#             resized_data = scale_semantic_map(data, (resolution, resolution))\n",
    "\n",
    "#             semantic_maps.append(resized_data[:,:,0])\n",
    "\n",
    "#         elif file_type == \"SA_ED\":\n",
    "#             img = nib.load(os.path.join(subject_path, file))\n",
    "#             data = img.get_fdata()\n",
    "\n",
    "#             for i in range(data.shape[2]):\n",
    "#                 resized_data = ndimage.zoom(data[:, :, i], (resolution/data.shape[0], resolution/data.shape[1]))\n",
    "#                 edges = detect_edges(resized_data)\n",
    "\n",
    "#                 scale = scale_data(resized_data)\n",
    "\n",
    "#                 normalized_images.append(scale)\n",
    "\n",
    "#                 edges_images.append(edges)\n",
    "\n",
    "#         elif file_type == \"SA_ED_gt\":\n",
    "#             img = nib.load(os.path.join(subject_path, file))\n",
    "#             data = img.get_fdata()\n",
    "\n",
    "#             for i in range(data.shape[2]):\n",
    "#                 resized_data = scale_semantic_map(data[:, :, i], (resolution, resolution))\n",
    "\n",
    "#                 semantic_maps.append(resized_data)\n",
    "\n",
    "#         elif file_type == \"SA_ES\":\n",
    "#             img = nib.load(os.path.join(subject_path, file))\n",
    "#             data = img.get_fdata()\n",
    "\n",
    "#             for i in range(data.shape[2]):\n",
    "#                 resized_data = ndimage.zoom(data[:, :, i], (resolution/data.shape[0], resolution/data.shape[1]))\n",
    "\n",
    "#                 edges = detect_edges(resized_data)\n",
    "\n",
    "#                 scale = scale_data(resized_data)\n",
    "\n",
    "#                 normalized_images.append(scale)\n",
    "\n",
    "#                 edges_images.append(edges)\n",
    "\n",
    "#         elif file_type == \"SA_ES_gt\":\n",
    "#             img = nib.load(os.path.join(subject_path, file))\n",
    "#             data = img.get_fdata()\n",
    "\n",
    "#             for i in range(data.shape[2]):\n",
    "#                 resized_data = scale_semantic_map(data[:, :, i], (resolution, resolution))\n",
    "\n",
    "#                 semantic_maps.append(resized_data)\n",
    "\n",
    "#     # Store the index range for this subject\n",
    "#     start_idx = current_index\n",
    "#     num_samples = len(normalized_images)\n",
    "#     end_idx = start_idx + num_samples - 1\n",
    "    \n",
    "#     index_mapping[sub] = {\n",
    "#         \"start_index\": start_idx,\n",
    "#         \"end_index\": end_idx,\n",
    "#         \"num_samples\": num_samples,\n",
    "#         \"file_name\": f\"{sub}_data.h5\"\n",
    "#     }\n",
    "    \n",
    "#     current_index = end_idx + 1\n",
    "\n",
    "#     # After processing all files for a subject, create the compressed HDF5 file\n",
    "#     hdf5_filename = f\"{sub}_data.h5\"\n",
    "#     hdf5_filepath = os.path.join(processed_data_path, hdf5_filename)\n",
    "#     with h5py.File(hdf5_filepath, 'w') as hf:\n",
    "#         hf.create_dataset('images', data=np.array(normalized_images), dtype=np.float32, compression=\"gzip\")\n",
    "#         hf.create_dataset('edges', data=np.array(edges_images), dtype=np.uint8, compression=\"gzip\")\n",
    "#         hf.create_dataset('semantic_maps', data=np.array(semantic_maps), dtype=np.uint8, compression=\"gzip\")\n",
    "\n",
    "#     print(f\"Created compressed HDF5 file for subject {sub}: {hdf5_filepath}\")\n",
    "\n",
    "# # Save the index mapping to a JSON file\n",
    "# mapping_filepath = os.path.join(processed_data_path, \"dataset_index_mapping.json\")\n",
    "# with open(mapping_filepath, 'w') as f:\n",
    "#     json.dump(index_mapping, f, indent=2)\n",
    "\n",
    "# print(f\"Created index mapping file: {mapping_filepath}\")\n",
    "# print(\"Finished processing all subjects.\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_path = \"MnM2/dataset\"\n",
    "\n",
    "resolution = 256\n",
    "\n",
    "empty_map = np.zeros((resolution, resolution), dtype=np.uint8)\n",
    "\n",
    "all_subjects = [d for d in os.listdir(subjects_path) if os.path.isdir(os.path.join(subjects_path, d))]\n",
    "\n",
    "sorted_subjects = sorted(all_subjects)\n",
    "\n",
    "processed_data_path = os.path.join(\"processed_data/autoencoder\")\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "# Dictionary to store the index mapping\n",
    "index_mapping = {}\n",
    "current_index = 0\n",
    "\n",
    "for sub in sorted_subjects:\n",
    "    subject_path = os.path.join(subjects_path, sub)\n",
    "    subject_files = [d for d in os.listdir(subject_path) if d.endswith(('.nii', '.nii.gz'))]\n",
    "    sorted_subject_files = sorted(subject_files)\n",
    "\n",
    "    print(sorted_subject_files)\n",
    "\n",
    "    normalized_images = []\n",
    "    edges_images = []\n",
    "    semantic_maps = []\n",
    "\n",
    "    for file in sorted_subject_files:\n",
    "        parts = file.split('_')\n",
    "        file_type = '_'.join(parts[1:-1])\n",
    "        file_type += '_' + parts[-1].split('.')[0]\n",
    "\n",
    "        if file_type == \"SA_ED\":\n",
    "            img = nib.load(os.path.join(subject_path, file))\n",
    "            data = img.get_fdata()\n",
    "\n",
    "            for i in range(data.shape[2]):\n",
    "                resized_data = ndimage.zoom(data[:, :, i], (resolution/data.shape[0], resolution/data.shape[1]))\n",
    "                edges = detect_edges(resized_data)\n",
    "\n",
    "                scale = scale_data(resized_data)\n",
    "\n",
    "                normalized_images.append(scale)\n",
    "\n",
    "                edges_images.append(edges)\n",
    "\n",
    "        elif file_type == \"SA_ED_gt\":\n",
    "            img = nib.load(os.path.join(subject_path, file))\n",
    "            data = img.get_fdata()\n",
    "\n",
    "            for i in range(data.shape[2]):\n",
    "                resized_data = scale_semantic_map(data[:, :, i], (resolution, resolution))\n",
    "\n",
    "                semantic_maps.append(resized_data)\n",
    "\n",
    "        elif file_type == \"SA_ES\":\n",
    "            img = nib.load(os.path.join(subject_path, file))\n",
    "            data = img.get_fdata()\n",
    "\n",
    "            for i in range(data.shape[2]):\n",
    "                resized_data = ndimage.zoom(data[:, :, i], (resolution/data.shape[0], resolution/data.shape[1]))\n",
    "\n",
    "                edges = detect_edges(resized_data)\n",
    "\n",
    "                scale = scale_data(resized_data)\n",
    "\n",
    "                normalized_images.append(scale)\n",
    "\n",
    "                edges_images.append(edges)\n",
    "\n",
    "        elif file_type == \"SA_ES_gt\":\n",
    "            img = nib.load(os.path.join(subject_path, file))\n",
    "            data = img.get_fdata()\n",
    "\n",
    "            for i in range(data.shape[2]):\n",
    "                resized_data = scale_semantic_map(data[:, :, i], (resolution, resolution))\n",
    "\n",
    "                semantic_maps.append(resized_data)\n",
    "\n",
    "        elif file_type == \"SA_CINE\":\n",
    "            img = nib.load(os.path.join(subject_path, file))\n",
    "            data = img.get_fdata()\n",
    "\n",
    "            for slice_idx in range(data.shape[2]):\n",
    "                for frame_idx in range(data.shape[3]):\n",
    "                    resized_data = ndimage.zoom(data[:, :, slice_idx, frame_idx], (resolution/data.shape[0], resolution/data.shape[1]))\n",
    "                    \n",
    "                    edges = detect_edges(resized_data)\n",
    "                    \n",
    "                    scale = scale_data(resized_data)\n",
    "\n",
    "                    normalized_images.append(scale)\n",
    "\n",
    "                    edges_images.append(edges)\n",
    "\n",
    "                    semantic_maps.append(empty_map)\n",
    "\n",
    "    # Store the index range for this subject\n",
    "    start_idx = current_index\n",
    "    num_samples = len(normalized_images)\n",
    "    end_idx = start_idx + num_samples - 1\n",
    "    \n",
    "    index_mapping[sub] = {\n",
    "        \"start_index\": start_idx,\n",
    "        \"end_index\": end_idx,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"file_name\": f\"{sub}_data.h5\"\n",
    "    }\n",
    "    \n",
    "    current_index = end_idx + 1\n",
    "\n",
    "    # After processing all files for a subject, create the compressed HDF5 file\n",
    "    hdf5_filename = f\"{sub}_data.h5\"\n",
    "    hdf5_filepath = os.path.join(processed_data_path, hdf5_filename)\n",
    "    with h5py.File(hdf5_filepath, 'w') as hf:\n",
    "        hf.create_dataset('images', data=np.array(normalized_images), dtype=np.float32, compression=\"gzip\")\n",
    "        hf.create_dataset('edges', data=np.array(edges_images), dtype=np.uint8, compression=\"gzip\")\n",
    "        hf.create_dataset('semantic_maps', data=np.array(semantic_maps), dtype=np.uint8, compression=\"gzip\")\n",
    "\n",
    "    print(f\"Created compressed HDF5 file for subject {sub}: {hdf5_filepath}\")\n",
    "\n",
    "# Save the index mapping to a JSON file\n",
    "mapping_filepath = os.path.join(processed_data_path, \"dataset_index_mapping.json\")\n",
    "with open(mapping_filepath, 'w') as f:\n",
    "    json.dump(index_mapping, f, indent=2)\n",
    "\n",
    "print(f\"Created index mapping file: {mapping_filepath}\")\n",
    "print(\"Finished processing all subjects.\")\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
